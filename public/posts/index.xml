<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Guangyao Zhao</title>
    <link>https://example.com/posts/</link>
    <description>Recent content in Posts on Guangyao Zhao</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 May 2022 21:09:12 -0500</lastBuildDate><atom:link href="https://example.com/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>常用latex符号输入</title>
      <link>https://example.com/posts/2022-02-15_latex_%E5%B8%B8%E7%94%A8latex%E7%AC%A6%E5%8F%B7%E8%BE%93%E5%85%A5/%E5%B8%B8%E7%94%A8latex%E7%AC%A6%E5%8F%B7%E8%BE%93%E5%85%A5/</link>
      <pubDate>Fri, 27 May 2022 21:09:12 -0500</pubDate>
      
      <guid>https://example.com/posts/2022-02-15_latex_%E5%B8%B8%E7%94%A8latex%E7%AC%A6%E5%8F%B7%E8%BE%93%E5%85%A5/%E5%B8%B8%E7%94%A8latex%E7%AC%A6%E5%8F%B7%E8%BE%93%E5%85%A5/</guid>
      <description>希腊字母 希腊字母 读音 小写 大写 读音 小写 大写 阿尔法 \alpha(\(\alpha\)) \Alpha(\(\Alpha\)) 纽 \nu(\(\nu\)) \Nu(\(\Nu\)) 贝塔 \beta(\(\beta\)) \Beta(\(\Beta\)) 柯西 \xi(\(\xi\)) \Xi(\(\Xi\)) 伽马 \gamma(\(\gamma\)) \Gamma(\(\Gamma\)) 奥密克戎 \omicro(\(\omicron\)) \Omicron(\(\Omicron\)) 德尔塔 \delta(\(\delta\)) \Delta(\(\Delta\)) 派 \pi(\(\pi\)) \Pi(\(\Pi\)) 艾普西隆 \epsilon(\(\epsilon\)) \Epsilon(\(\Epsilon\)) 柔 \rho(\(\rho\)) \Rho(\(\Rho\)) 截塔 \zeta(\(\zeta\)) \Zeta(\(\zeta\)) 西格玛 \sigma(\(\sigma\)) \Sigma(\(\Sigma\)) 艾塔 \eta(\(\eta\)) \Eta(\(\Eta\)) 淘 \tau(\(\tau\)) \Tau(\(\Tau\)) 西塔 \theta(\(\theta\)) \Theta(\(\Theta\)) 宇普西隆 \upsilon(\(\upsilon\)) \Upsilon(\(\Upsilon\)) 约塔 \iota(\(\iota\)) \Iota(\(\Iota\)) 斐 \phi(\(\phi\)) \Phi(\(\Phi\)) 卡帕 \kappa(\(\kappa\)) \Kappa(\(\Kappa\)) 西 chi(\(\chi\)) \Chi(\(\Chi\)) 拉姆达 \lambda(\(\lambda\)) \Lambda(\(\Lambda\)) 普西 \psi(\(\psi\)) \Psi(\(\Psi\)) 米尤 \mu(\(\mu\)) \Mu(\(\Mu\)) 奥米伽 \omega(\(\omega\)) \Omega(\(\Omega\)) 希腊字母的一些变化 变量 \varGamma(\(\varGamma\)) \varlambda(\(\varLambda\)) \varSigma(\(\varSigma\)) \varPsi(\(\varPsi\)) \varDelta(\(\varDelta\)) \varXi(\(\varXi\)) \varUpsilon(\(\varUpsilon\)) \varOmega(\(\varOmega\)) \varTheta(\(\varTheta\)) \varPi(\(\varPi\)) \varPhi(\(\varPhi\)) 直立 在 latex 中需要添加upgreek宏包。</description>
    </item>
    
    <item>
      <title>最大似然估计(maximum likelihood estimation, MLE)</title>
      <link>https://example.com/posts/2022-02-25_ml_%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</link>
      <pubDate>Fri, 25 Feb 2022 21:13:14 -0500</pubDate>
      
      <guid>https://example.com/posts/2022-02-25_ml_%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</guid>
      <description>似然函数 似然性（likelihood）和概率（possibility）同样可以表示事件发生的可能性大小，但是两者有很大区别：
概率：是在已知参数\(\theta\)的情况下，发生观测结果\(x\)可能性大小。
似然性：从观测结果\(x\)出发，分布函数的参数\(\theta\)的可能性大小。
似然函数如下：
\[ \mathrm{L}(\theta|x)=p(x|\theta) \]
其中：\(x\)已知，\(\theta\)未知。若对于两个参数\(\theta_1\)和\(\theta_2\)，有： \[ \mathrm{L}(\theta_1|x)=p(x|\theta_1) &amp;gt; p(x|\theta_2) = \mathrm{L}(\theta_2|x) \]
那么意味着\(\theta=\theta_1\)的时候，随机变量\(X\)生成\(x\)的概率大于当参数\(\theta=\theta_2\)。这也正是似然函数的意义所在。若观测数据为\(x\)，那么\(\theta_1\)比\(\theta_2\)更有可能是分布函数的参数。
最大似然估计 最大似然函数的思想在于，对于给定观测数据\(x\)，希望能反推出所有参数\(\theta_1,\theta_2,...,\theta_k\)中找出能最大概率生成观测数据的参数\(\theta^*\)作为估计结果。即，被估计的参数\(\theta^*\)应该满足：
\[ \mathrm{L}(\theta^*|x)=p(x|\theta^*) &amp;gt; p(x|\theta) = \mathrm{L}(\theta|x),\theta=\theta_1,...,\theta_k \]
在实际运算中，将待估参数\(\theta\)作为变量，计算生成观测数据\(x\)的概率函数\(p(x|\theta)\)，并通过求导找到最大概率函数的参数即可：
\[ \theta^* = \underset{\theta}{\mathrm{argmax}}\,p(x|\theta) \]
离散型随机变量的最大似然估计 在参数\(\theta\)下，分布函数随机取到\(x_1,x_2,...,x_n\)的概率为：
\[ p(x|\theta)=\prod_{i=1}^{n}p(x_i;\theta) \] 构造似然函数：
\[ \mathrm{L}(\theta|x) = p(x|\theta)=\prod_{i=1}^{n}p(x_i;\theta) \]
似然函数是一个关于\(\theta\)的函数，要找到最大概率生成\(x\)的参数，即需要找到\(\mathrm{L}(\theta|x)\)取最大值时的\(\theta\)。此时需要对其求导：
\[ \frac{d}{d\theta}\mathrm{L}(\theta|x)=0 \]
因为很一般情况下式子是累积形式，所以可借助对数函数简化问题：
\[ \frac{d}{d\theta}\ln(\mathrm{L}(\theta|x))=0 \]
上式通常称作对数似然方程。如果包含多个参数\(\theta_1, \theta_2,...,\theta_k\)，则可对多个参数分别求导联立方程组。
高斯分布下的最大似然函数 假设样本符合高斯分布，即\(X\sim N(\mu,\sigma^2)\)，其中\(\mu\)为均值，\(\sigma\)为方差。\(x_1,x_2,...x_n\)为来自\(X\)的一组观察值，求\(\mu\)和\(\sigma\)的最大似然估计。
\(X\)的概率密度函数：
\[ f(x;\mu,\sigma) = \frac{1}{\sqrt{2\pi}\sigma}\exp{-\frac{(x-\mu)^2}{2\sigma^2}} \]
观察值\(x_1,x_2,...x_n\)的似然函数为：
\[ \begin{aligned} \mathrm{L}(\theta|x) &amp;amp;= \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}\exp{-\frac{(x_i-\mu)^2}{2\sigma^2}}\\ \ln{\mathrm{L}(\theta|x)} &amp;amp;= -n\ln{\sqrt{2\pi}\sigma} - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i-\mu)^2 \end{aligned} \]</description>
    </item>
    
    <item>
      <title>向量概念的几何意义</title>
      <link>https://example.com/posts/2022-02-07_la_%E5%90%91%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%87%A0%E4%BD%95%E6%84%8F%E4%B9%89/%E5%90%91%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%87%A0%E4%BD%95%E6%84%8F%E4%B9%89/</link>
      <pubDate>Tue, 15 Feb 2022 21:13:14 -0500</pubDate>
      
      <guid>https://example.com/posts/2022-02-07_la_%E5%90%91%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%87%A0%E4%BD%95%E6%84%8F%E4%B9%89/%E5%90%91%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%87%A0%E4%BD%95%E6%84%8F%E4%B9%89/</guid>
      <description>自由向量的概念 向量是一个既有大小又有方向的量，这个量本身就是个几何的概念。 在物理学中把向量叫做矢量。矢就是箭，向量如同一根箭一样有头部和尾部，箭在空间自由的飞行中箭杆的长度不会变，这一点和向量相同；同时箭在无重力作用的理想情况下方向不会改变。也就是说长度和方向不变的理想之箭就是一个向量。所以向量的”飞行”称之为平移，这种允许在一条直线上平移的向量称为自由向量。 物理界将 vector 称之为矢量，数学界称之为向量。 向量的几何表示为\(\boldsymbol{AB}\)；代数表示为\(\boldsymbol{a}\)；手写时因为表示困难，所以写为\(\boldsymbol{a}\) 虽然向量独立于任何坐标系之外，但是为了与解析技术联系起来实现对向量的计算，数学上还是必须将向量放在某一个坐标系下研究。如果把空间中所有的向量的尾部都拉到坐标原点，这样\(n\)维空间就可以和\(n\)维向量空间建立一一对应的关系。 一旦确定好坐标系，一个向量就是与一个点对应，而点是所谓坐标的有序数组表示的，因此就可以把向量用有序数组表示，有了有序数组就可以运算了。使用有序数组表述的向量是以原点为起点的向量末端的坐标值表示，并把坐标值用圆括号括起来，如\(\boldsymbol{a}=(x,y,z)\)。在此处的有序数组\((x,y,z)\)就是向量。 一个向量可以分解为三个单位坐标向量的线性表示，比如向量\((1,1,1)\)分解如下：\((1,1,1)=(1,0,0)+(0,1,0)+(0,0,1)=\boldsymbol{i}+\boldsymbol{j}+\boldsymbol{k}\) 任意一个向量都可以表示为\(\boldsymbol{a}=(x,y,z)=x\boldsymbol{i}+y\boldsymbol{j}+z\boldsymbol{k}\) 向量的运算有加法、减法和乘法。乘法又分为点积和叉积。除法少有提及，后面会解释原因。 向量被看做线性空间或向量空间中的一个元素，但是与点不同。向量表示的是两点之间的位移而不是具体的空间的物理位置，是独立于坐标系的，这就是为什么在描述向量的运算法则的时候不需要画出坐标系，但一个点离开坐标系就无法表示。 向量实际上使用一个点对表示的，比如\(\boldsymbol{AB}\)表示起点为\(A\)终点为\(B\)，之所以把一个点与一个向量相对应，是因为默认所有的向量都是从原点出发的。 向量内积的几何和物理意义 向量内积的几何解释 向量的内积(dot product)也称之为数量积、标积、点积。内积的结果是个标量，定义如下： \[ \begin{aligned} \boldsymbol{a}\cdot\boldsymbol{b}&amp;amp;=ab\cos \theta\\ \boldsymbol{a}\cdot\boldsymbol{b}&amp;amp;=a_xb_x+a_yb_y+a_zb_z \end{aligned} \]
根据内积的定义可对向量求长度： \[ a = \sqrt{\boldsymbol{a}\cdot\boldsymbol{a}}=\sqrt{a_xa_x+a_ya_y+a_za_z}=\sqrt{a_x^2+a_y^2+a_z^2} \]
那么内积的两个定义有几何关系吗？答案是有的。假设\(a_y\)和\(a_z\)均为\(0\)，那么：\(\boldsymbol{a}\cdot\boldsymbol{b}=ab_x\)，其中\(ab_x\)的含义为\(\boldsymbol{a}\)的长度乘以\(\boldsymbol{b}\)在\(\boldsymbol{a}\)方向上的分量，即投影。投影表示为\(b_x=b\cos \theta\)。因此\(\boldsymbol{a}\cdot\boldsymbol{b}=ab\cos \theta\)得以证明。
向量内积的几何解释就是一个向量在另一个向量上的投影的积，也就是同方向的积。
如果想要将一个向量变换到新的坐标系，只需要对新坐标系轴向量进行内积运算即可（这个理论极其重要）。
内积还有一种比较直观的解释：
当两个向量的内积\(&amp;gt;0\)时，同方向 当两个向量的内积\(=0\)时，互相垂直 当两个向量的内积\(&amp;lt;0\)时，反方向 向量内积的物理解释 生活中的内积：比如食物的价格分别为蔬菜\(2\)元；大米\(1.5\)元；牛肉\(10\)元，那么价格向量为\(p=(2,1.5,10)\)。重量分别为蔬菜\(3.5\)斤；大米\(5\)斤；牛肉\(2\)斤，那么重量向量为\(d=(3.5,5,2)\)。饮食消费的内积为： \[ \boldsymbol{p}\cdot\boldsymbol{d}=(2,1.5,10)\cdot(3.5,5,2)=34.5 \]
向量求解余弦插角公式：假设\(\boldsymbol{A}=(\cos \alpha, \sin\alpha)\)，\(\boldsymbol{B}=(\cos \beta, \sin\beta)\)，则： \[ \begin{aligned} \boldsymbol{OA}\cdot\boldsymbol{OB}=|\boldsymbol{OA}|\cdot|\boldsymbol{OB}|\cos(\beta-\alpha)=\cos\alpha\cos\beta+\sin\alpha\sin\beta\\ \cos(\beta-\alpha) = \cos\alpha\cos\beta+\sin\alpha\sin\beta \quad (|\boldsymbol{OA}|=\boldsymbol{OB}=1) \end{aligned} \]
向量叉积的几何和物理意义 叉积的定义及其几何解释 叉积(cross product)也称之为外积，因为叉积或产生新的一维向量。两个向量确定了一个二维的平面，叉积会产生垂直于这个平面的向量。 叉积的定义也有两个： \[ \begin{aligned} \boldsymbol{a}\times \boldsymbol{b}&amp;amp;=(ab\sin\theta)\boldsymbol{n_0}\\ \boldsymbol{a}\times \boldsymbol{b}&amp;amp;=(a_yb_z-a_zb_y,a_zb_x-a_xb_z,a_xb_y-z_yb_x) \end{aligned} \]</description>
    </item>
    
    <item>
      <title>最小二乘法背后的高斯噪声(Gaussian method)思想</title>
      <link>https://example.com/posts/2022-02-03_ml_%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0/%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0/</link>
      <pubDate>Thu, 03 Feb 2022 21:09:12 -0500</pubDate>
      
      <guid>https://example.com/posts/2022-02-03_ml_%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0/%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0/</guid>
      <description>高斯噪声指的是概率密度函数服从高斯分布的一类噪声：\(\epsilon \sim N(0,\sigma^2)\)。在线性回归中，隐藏的一个假设就是噪声（预测值和真实值之差）服从高斯分布： \[ \begin{aligned} y_i &amp;amp;= \hat{y_i} + \epsilon\\ &amp;amp;= w^\mathrm{T}x_i + \epsilon \end{aligned} \] 即：\(y_i|x_i;w \sim N(w^\mathrm{T}x,\sigma^2)\)。利用最大似然估计： \[ \begin{aligned} L(w|\mathbf{X})=\ln{p(\mathbf{X}|w)} &amp;amp;=\ln{\prod_{i=1}^{n}{p(y_i|x_i;w)}}=\sum_{i=1}^{n}\ln{p(y_i|x_i;w)}\\ &amp;amp;=\sum_{i=1}^{n}\left ( \ln{\frac{1}{\sqrt{2\pi}\sigma}} -\frac{(y_i-w^\mathrm{T}x_i)^2}{2\sigma^2}\right )\\ &amp;amp;= -\sum_{i=1}^{n}{(y_i-w^\mathrm{T}x_i)^2} \end{aligned} \] 求得目标函数： \[ \begin{aligned} \hat{w}&amp;amp;=\underset{w}{\mathrm{argmax}} -\sum_{i=1}^{n}{(y_i-w^\mathrm{T}x_i)^2}\\ &amp;amp;= \underset{w}{\mathrm{argmin}} \sum_{i=1}^{n}{(y_i-w^\mathrm{T}x_i)^2}\\ &amp;amp; = \underset{w}{\mathrm{argmin}} \sum_{i=1}^{n}{(y_i-\hat{y})^2}\\ \end{aligned} \]</description>
    </item>
    
    <item>
      <title>只有青春期，没有青春</title>
      <link>https://example.com/posts/2022-01-01_essay_%E5%8F%AA%E6%9C%89%E9%9D%92%E6%98%A5%E6%9C%9F%E6%B2%A1%E6%9C%89%E9%9D%92%E6%98%A5/%E5%8F%AA%E6%9C%89%E9%9D%92%E6%98%A5%E6%9C%9F%E6%B2%A1%E6%9C%89%E9%9D%92%E6%98%A5/</link>
      <pubDate>Sat, 01 Jan 2022 21:09:12 -0500</pubDate>
      
      <guid>https://example.com/posts/2022-01-01_essay_%E5%8F%AA%E6%9C%89%E9%9D%92%E6%98%A5%E6%9C%9F%E6%B2%A1%E6%9C%89%E9%9D%92%E6%98%A5/%E5%8F%AA%E6%9C%89%E9%9D%92%E6%98%A5%E6%9C%9F%E6%B2%A1%E6%9C%89%E9%9D%92%E6%98%A5/</guid>
      <description>写于2013年 02月20，于东莞某电子厂。
那一年，你和从小穿开裆裤的哥们儿还能在一个学校 你们一起上学 一起骑着并不高档的自行车 在坑坑洼洼的大路上 向家的那个方向飙车
那一年，其中一个哥们儿结婚了 你坐在桌前，抬了抬头 努力的让自己的思想 从冗杂的公式间游离而出 夕阳西沉，叶子悠悠的从树间滑落 你向窗外望了望说不着急
那一年，你上了大学 哥们儿的孩子已咿咿呀呀学语 你说大学的生活很自在 里面没有唠叨 也没有分数的担忧 你说自己可以无拘束的游戏、睡觉 却没有发现自己已不小
那一年，你说你其实也厌倦这种生活 你说要看很多很多的书 于是你泡在图书馆一天 找遍了别人口中所谓的名著 你木讷的看完了这些 却说不出自己有什么所获
那一年，国与国不太和平 你高喊抵制日货 小日本算什么 我们有五千年的灿烂历史 可当日本人努力的汲取中国文化的时候 你却连《论语》、《道德经》都没看过
那一年，你看到了很多的不公平 富欺穷，官压民的例子多不胜数 朋友们在高呼平等 你却旁若无人的沉迷游戏 笑话他们的无用功
那一年，你也想像别人一样出去走走 于是你向别人表示了自己的决心 时过多日 一个久别的老友问你都去了哪了 你不好意思的对他说 爸妈哪都不让你去
那一年，你回了趟家乡 父母的双鬓已多了些许白发 你看到哥们儿的孩子在满大街跑 他们冲你做了个鬼脸 你还没来得及回应 幼小的身影就消散出你的视线
你无意间走到了昔日的学校 看着满脸斗志的学弟学 你仿佛看到了当年的自己 回首离别的日子 你不忍心告诉他们什么 只是黯然的对自己说 我走过了青春期 却从没能拥有过青春</description>
    </item>
    
    <item>
      <title>Markdown Syntax Guide</title>
      <link>https://example.com/posts/markdown-syntax/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/posts/markdown-syntax/</guid>
      <description>&lt;p&gt;This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Emoji Support</title>
      <link>https://example.com/posts/emoji-support/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/posts/emoji-support/</guid>
      <description>&lt;p&gt;Emoji can be enabled in a Hugo project in a number of ways.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Placeholder Text</title>
      <link>https://example.com/posts/placeholder-text/</link>
      <pubDate>Mon, 18 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/posts/placeholder-text/</guid>
      <description>&lt;p&gt;Lorem est tota propiore conpellat pectoribus de
pectora summo.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
